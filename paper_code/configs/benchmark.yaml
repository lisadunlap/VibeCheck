project: VibeCheck-Benchmark
wandb: False # Set to True to log to Weights and Biases
num_samples: False
dummy_eval: False
oz: False
group_column: False
test: False
# data_path: data/benchmark/best_chatgpt_prompts_train.csv
data_path: data/benchmark/best_chatgpt_prompts_biiiiger.csv
models: ['baseline', 'critical']
run_lr: False

k: 5
batch_size: 50
num_axes_generated: 10
num_eval: 10
embedding_model: text-embedding-3-small
seed: 42
cluster_method: hierarchical
ranker: RelativeRanker

sampler: Sampler
reducer: AxisReducer
proposer: LLMProposerFixed
num_topic_clusters: 5
proposer_batch_size: 5

judges: [gpt-4o-mini]
eval_only: False
num_proposal_samples: 20
new_prompt: True
axes: False

new_sample: True

# models
rubric_generation_model: "gpt-4o"
proposer_model: "gpt-4o"